# '''
# first step:
# pre-train the coarse model (i.e.ğ¸ğ‘) for two epochs with a batch size of 64,
# with ğœ†ğ‘™ğ‘šğ‘˜=1ğ‘’âˆ’4, ğœ†ğ‘’ğ‘¦ğ‘’=1.0, ğœ†ğœ·=1ğ‘’âˆ’4, and ğœ†ğ=1ğ‘’âˆ’4

# Why:
# training with only lmk loss for good initialization, 
# because the use of photometric loss needs good initialization both in regression and optimization
# and also, photometric loss needs differentiable rendering that makes the training slow
# 
# 
# '''

 # 1080ti
 output_dir: "/home/wang/SSD_2/demo/DECA_TR/pretrain_1080_8_235_0105"
 pretrained_modelpath: ''

# 4090
#output_dir: "/media/c723/SSD_2/demo/DECA/train/step_1_pretrain"
#pretrained_modelpath: '/media/c723/SSD_2/demo/DECA/data/deca_model.tar'


dataset:
  batch_size: 8
  K: 1
loss:
  photo: 2.0
  id: 0.
  useSeg: False
  shape_consistency: False
  lmk_num: 191
  eyed: 0.  #1.0
  lipd: 0.5  #0.3

  reg_light: 0.
  reg_tex: 0.




train:
  resume: True
  max_epochs: 100
  max_steps: 10000000
  log_steps: 100
  vis_steps: 200
  checkpoint_steps: 500
  val_steps: 300
  eval_steps: 10000
model:
  E_flame_backbone: 'convnext_v2_tiny'
